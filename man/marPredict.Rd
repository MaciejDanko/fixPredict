% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/newpredict.R
\name{marPredict}
\alias{marPredict}
\title{Unbiased predictions for fixed variables}
\usage{
marPredict(object, newdata, type = c("link", "response"),
  ci.fit = TRUE, alpha = 0.95, n.sims = 1000,
  average.missing = FALSE, sim.missing = TRUE, sim.RE = FALSE,
  approx.RE = FALSE, method = c("at.each.cat", "overall"),
  use.offset = TRUE, as.rate = FALSE, return.median = FALSE,
  return.boot = FALSE)
}
\arguments{
\item{object}{a object of class inherited from: \code{\link{glmer}{lme4}},
\code{\link{gamm}{mgcv}}, \code{\link{gam}{mgcv}}, \code{\link{gamm4}{gamm4}}, \code{\link{glmer.nb}{lme4}}, \code{\link{lmer}{lme4}}, \code{\link{lme}{nlme}},
\code{\link{glm}{stats}}, \code{\link{lm}{stats}}, or \code{\link{glm.nb}{MASS}}.}

\item{newdata}{an optional data frame in which to look for variables with which to predict. If omitted, the fitted values are used.}

\item{type}{the type of prediction required, either "\code{link}" or "\code{response}".
The default is on the scale of the linear predictors; the alternative "response" is on the scale of the response variable.
see also \code{\link{predict.glm}{stats}}.}

\item{ci.fit}{a logical indicting if to bootstrap percentile confidence intervals.}

\item{alpha, n.sims}{a significance level and a number of repetitions for the bootstrap method.}

\item{average.missing}{a logical indicating if to perform population averaging over the variables missing from the \code{newdata}.}

\item{sim.missing}{a logical indicating if to simulate uncertenity of the missing variables via the bootstrap method. Miningfull only when \code{average.missing} is \code{TRUE}.}

\item{sim.RE}{a logical indicating if to simulate uncertenity of the random effects via the bootstrap method.}

\item{approx.RE}{logical indicating if to use infinite-subjects approximation for random efect contribution to the marginalized model predictions.}

\item{method}{a method of averaging over random effects.
The default "\code{at.each.cat}" estimate avarage random effects at each cobination of levels for categorical variable(s) present in the \code{newdata};
the alternative "\code{overall}" estimate average random effects for total population.}

\item{use.offset}{a logical indicating if to include averaged offset into model predictions.}

\item{as.rate}{a logical indicating if to return predicitions as rates. Setting this \code{TRUE} makes sense only for count models (e.g., Poisson, neg.bin).}

\item{return.median}{a logical indicating if to return median for bootstrap percentile method.}

\item{return.boot}{a logical indicating if to return bootstraped matrices.}
}
\value{
a list with model predicitions with the following components:
\item{fit}{ a list with \code{biased} (random effects excluded) and \code{unbiased}(averaged random effects included) predicitions.}
\item{CI.fit}{ a optional list with bootstraped confidence intervals for \code{biased} and \code{unbiased} predicitions.}
\item{boot}{ a optional list with bootstraped matrices for \code{biased} and \code{unbiased} predicitions.}
\item{offset}{ the offset used for model predictions.}
}
\description{
Unbiased predictions for fixed variables
}
\examples{
\donttest{
#############################################################################
# EXAMPLE 1
#############################################################################

set.seed(3)
data1 <- sim_glmer_data(formula = ~ X1 * X2, theta = 0.75,
                         coef= c(5, 0.5, 0.7, 0.5, -0.7, 0.2, 0.3, 0.3, 0.2),
                         n.levels=c(3,3), n.ID = 100, n.pop=1e3)

fit1 <- lme4::glmer(Y~X1*X2+(1|ID),family=poisson(), data=data1)

DATA <- with(data1, tapply(Y, data.frame(X1=X1,X2=X2), mean))
posX <- as.vector(barplot(DATA,beside=TRUE,ylim=c(0,1050),
                          ylab='Mean counts (response scale)'))
text(posX,-40,rep(rownames(DATA),3),xpd=TRUE)
axis(1,at=c(-5,100),labels = FALSE)

newdata1 <- expand.grid(X1=levels(data1$X1), X2=levels(data1$X2))

# calculate the biased and the unbiased predictions 
# on the response scale
cYA <- marPredict(object=fit1,
                  newdata=as.data.frame(newdata1),
                  type='response',
                  ci.fit = TRUE,
                  method = 'at.each.cat')

# using overall method
cYB <- marPredict(object=fit1,
                  newdata=as.data.frame(newdata1),
                  type='response',
                  ci.fit = TRUE,
                  method = 'overall')

# plot the predicited responses
lines(posX-0.3,cYA$fit$biased,pch=20,type='p',col='red',cex=1.5)
lines(posX+0.3,cYA$fit$unbiased,pch=20,type='p',
      col=rgb(0,180,0,maxColorValue = 255),cex=1.5)
lines(posX,cYB$fit$unbiased,pch=20,type='p',col='darkorange',cex=1.5)

# plot the confidence intervals (the bootstrap percentile method)
for (i in seq_along(posX)) lines(c(posX[i],posX[i])+0.3,
                                 cYA$CI.fit$unbiased[i,], 
                                 col=rgb(0,180,0,maxColorValue = 255))
for (i in seq_along(posX)) lines(c(posX[i],posX[i])-0.3,
                                 cYA$CI.fit$biased[i,], 
                                 col='red')
for (i in seq_along(posX)) lines(c(posX[i],posX[i]),
                                 cYB$CI.fit$unbiased[i,], 
                                 col='darkorange')

legend('topleft',c('Simulated data','RE excluded',
                   'RE averaged',
                   'RE category-averaged'),
       pch=c(NA,19,19,19),pt.cex=c(1,1.5,1.5,1.5),
       fill=c(gray.colors(3)[2],NA,NA,NA),
       col=c(NA,2,'darkorange',rgb(0,180,0,maxColorValue = 255)), 
       bty='n', border=c('black',NA,NA,NA))
        
#############################################################################
# EXAMPLE 2
#############################################################################
data2 <- sim_glmer_data(formula = ~ X1, theta = 0.75,
                        coef= c(3, 0.5, -0.7),
                        coef.c = 5,
                        func.c = function(C1, coef.c) 
                          abs(C1)^1.5 * coef.c - 1.5,
                        n.levels=3, n.ID = 100, n.pop=1e3)

# fitting the model
fit2 <- gamm4::gamm4(Y~ X1 + s(C1), random = ~(1|ID), 
                     family = poisson(), data = data2)

# making the predictions
newdata2 <- expand.grid(C1=seq(-0.5,0.5,0.05), X1=levels(data1$X1))
cY2 <- marPredict(fit2, newdata2, type = 'response', ci.fit = TRUE)

# plotting the results
ina <- which(newdata2$X1=='a'); inb <- which(newdata2$X1=='b')
inc <- which(newdata2$X1=='c')
xa <- newdata2$C1[ina]; xb <- newdata2$C1[inb]; xc <- newdata2$C1[inc]

CIplot.ci(xa, cY2$fit$unbiased[ina], cY2$CI.fit$unbiased[ina,],
          density=40, angle=0,first = TRUE,ylim=c(0,60),col='blue',lwd=4,
          xaxt='s', yaxt='s', ylab='Mean counts (response scale)', xlab='C1')
CIplot.ci(xa, cY2$fit$unbiased[inb], cY2$CI.fit$unbiased[inb,],
          density=40, angle=0,col='red',lwd=4)
CIplot.ci(xa, cY2$fit$unbiased[inc], cY2$CI.fit$unbiased[inc,],
          density=40, angle=0,col=rgb(0,180,0,maxColorValue = 255),lwd=4)
CIplot.ci(xa, cY2$fit$biased[ina], cY2$CI.fit$biased[ina,],
          density=20, angle=90,col='blue',lty=2)
CIplot.ci(xa, cY2$fit$biased[inb], cY2$CI.fit$biased[inb,],
          density=20, angle=90,col='red',lty=2)
CIplot.ci(xa, cY2$fit$biased[inc], cY2$CI.fit$biased[inc,],
          density=20, angle=90,col=rgb(0,180,0,maxColorValue = 255),lty=2)

legend('top',c('biased','unbiased','a','b','c'), lty=c(2,1,1,1,1),
       lwd=c(1,4,2,2,2),col=c(1,1,'blue','red',
                              rgb(0,180,0,maxColorValue = 255)),bty='n')
                              
#############################################################################
# EXAMPLE 3
#############################################################################
data3 <- sim_glmer_data(formula = ~ X1, theta = 1.1,
                        coef= c(0.5, 1.5, -1.25),
                        coef.c = 2, s=0.3,
                        func.c = function(C1, coef.c) C1* coef.c,
                        n.levels=3, n.ID = 100, n.pop=1e3,
                        family='binomial')

ina <- which(data3$X1=='a'); inb <- which(data3$X1=='b')
inc <- which(data3$X1=='c')
plot(data3$C1[ina],binomial()$linkinv(data3$linkY)[ina],
     ylim=c(0,1), cex=0.2, col = 'blue')
lines(data3$C1[inb],binomial()$linkinv(data3$linkY)[inb],
      type='p', cex=0.2, col = 'red')
lines(data3$C1[inc],binomial()$linkinv(data3$linkY)[inc],
      type='p', cex=0.2, col = rgb(0,180,0,maxColorValue = 255))

fit3 <- glmer(Y ~ X1 + C1 + (1|ID), family=binomial(), data=data3, nAGQ = 20)
cY3 <- marPredict(fit3, newdata3, type = 'response', ci.fit = TRUE)

newdata3 <- expand.grid(C1=seq(-0.5,0.5,0.05), X1=levels(data3$X1))

ina <- which(newdata3$X1=='a'); inb <- which(newdata3$X1=='b')
inc <- which(newdata3$X1=='c')
xa <- newdata3$C1[ina]; xb <- newdata3$C1[inb]; xc <- newdata3$C1[inc]

CIplot.ci(xa, cY3$fit$unbiased[ina], cY3$CI.fit$unbiased[ina,],
          density=40, angle=0,first = TRUE,ylim=c(0,1),col='blue',lwd=4,
          xaxt='s', yaxt='s', ylab='Probabilyty (response scale)', xlab='C1')
CIplot.ci(xa, cY3$fit$unbiased[inb], cY3$CI.fit$unbiased[inb,],
          density=40, angle=0,col='red',lwd=4)
CIplot.ci(xa, cY3$fit$unbiased[inc], cY3$CI.fit$unbiased[inc,],
          density=40, angle=0,col=rgb(0,180,0,maxColorValue = 255),lwd=4)
CIplot.ci(xa, cY3$fit$biased[ina], cY3$CI.fit$biased[ina,],
          density=20, angle=90,col='blue',lty=2)
CIplot.ci(xa, cY3$fit$biased[inb], cY3$CI.fit$biased[inb,],
          density=20, angle=90,col='red',lty=2)
CIplot.ci(xa, cY3$fit$biased[inc], cY3$CI.fit$biased[inc,],
          density=20, angle=90,col=rgb(0,180,0,maxColorValue = 255),lty=2)

legend('top',c('Biased pred. response', 'Unbiased pred. response',
               'a','b','c'), lty=c(2,1,1,1,1), lwd=c(1,4,2,2,2), 
       col=c(1,1,'blue','red', rgb(0,180,0,maxColorValue = 255)), bty='n')
                              
#############################################################################
# EXAMPLE 4
#############################################################################
#dontrun
#cat(1)
}
}
\author{
Maciej J. Danko
}
